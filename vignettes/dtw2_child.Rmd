## Introduction

Working with a *set* of time series measuring related observations
requires a different set of tools compared to analyzing or forecasting
a single time series.

If you want to cluster time series into groups with similar behaviors, 
one option is feature extraction: statistical summaries that
characterize some feature of the time series, such as min, max, or
spectral density. The [feasts](https://feasts.tidyverts.org/index.html) R package
and the Python package [tsfresh](https://github.com/blue-yonder/tsfresh)
provide tools to make this easier. 

Why not cluster on the time series directly? Standard methods don't work
as well, and can produce clusters that miss structure you can visually
identify as "similar". 

Dynamic time warping is method that aligns with intuitive notions of 
time series similarity. To show how it works, I'll walk through

1. how standard distance metrics fail to create useful time series clusters

2. dynamic time warping distance as a method for similarity

3. clustering similar time series

## Distance Metrics

```{r}
library(tsrecipes)
library(tidyverse)
library(dtwclust)
library(patchwork)
library(recipes)
```

To cluster, we need to measure the distance between every member of the group.^[
The UC Business Analytics 
[R Programming Guide](https://uc-r.github.io/hc_clustering) has an excellent
series on clustering, covering dissimilarity measures to the final clustering
algorithms.]
Typically we think of [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance#:~:text=In%20mathematics%2C%20the%20Euclidean%20distance,metric%20as%20the%20Pythagorean%20metric.):
the length of a straight line between two points.

This distance pops up all the time in data science,
usually in Mean Squared Error (MSE) or 
it's counterpart Root Mean Squared Error (RMSE). 
It's used to measure regression error in machine learning,
and assess the accuracy of a [time series forecast](https://otexts.com/fpp3/accuracy.html).

```{r, echo=FALSE}
library(fpp3)
library(tsibble)
library(tsibbledata)

aus_train <- tsibbledata::aus_production %>% filter_index("1992 Q1" ~ "2006 Q4")

beer_fit <- aus_train %>%
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer)
  )

beer_fc <- beer_fit %>% forecast(h=14)

beer_fc %>%
  autoplot(filter_index(aus_train, "2007 Q1" ~ .), level = NULL) +
  autolayer(filter_index(aus_production, "2007 Q1" ~ .), color = "black") +
  labs(x = NULL, y = NULL) +
  guides(colour=guide_legend(title="Forecast"))
```

To evaluate the fit of the forecast to the actual data, 
you can calculate the Euclidean distance between the corresponding points
in the time series and the forecasts. The smaller the distance, 
the better the forecast: the more *similar* the two series are.


A straight line between two points isn't always the possible. 
In a city grid, we are constrained by the blocks. In this situation, the distance
between two points is called the [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry).

```{r}
knitr::include_graphics("283px-Manhattan_distance.svg.png")
```

Time series also
need a special distance metric. The most common is called Dynamic Time Warping.

### Time Series Distance

Plotted below are three time series. I've plotted blue and green to both
overlap red. Is blue or green more similar to red?

```{r}
eth_sample <- ethanol %>%
  filter(id %in% c(1, 2, 53))

eth_sample_unnested_trunc <- eth_sample %>%
  mutate(n = list(1:1751)) %>%
  unnest(c(ts, n)) %>%
  filter(between(n, 500, 1500))

eth_sample_trunc <- eth_sample_unnested_trunc %>%
  select(-n) %>%
  group_by(id, class) %>%
  summarise(ts = list(ts), .groups = "drop")
```


```{r}
plot_overlap <- function(ids, x) {
  group_colors <- c(`1` = "#F8766D", `2` = "#619CFF")
  if (any(ids != c(1, 2))) {
    group_colors <- c(`1` = "#F8766D", `53` = "#00BA38")
  }
  
  x %>%
    filter(id %in% ids) %>%
    ggplot(aes(n, ts, color = as.factor(id), group = id)) +
    geom_line(show.legend = FALSE) +
    scale_color_manual(values = group_colors) +
    labs(x = NULL, y = NULL)
}

plots <- list(c(1, 2), c(1, 53)) %>%
  map(plot_overlap, eth_sample_unnested_trunc)

plots[[1]] / plots[[2]]
```

I think it's blue: blue and red both has an early dip after 750. 
Around 1000 they both have a slim, deep trough. The major difference is that
blue seems shifted to the left.

Green is all wrong: where red dips around 750, green has a bump. 
And the dip after 1000 is wider and shallower. 

The Euclidean distance tells a different story. 
Red is actually closer to green, because it has a smaller distance metric 
(9.78 vs 9.83).

```{r}
eth_sample$ts %>% 
  set_names(c("red", "blue", "green")) %>%
  simplify2array() %>% t() %>%
  dist()
```

## Dynamic Time Warping

To capture our intuition about the similarity of red and blue,
we need a new metric. This metric can't simply measure the point-to-point 
distance between the series.
As we saw, blue is shifted to the left of red, even though the shape
is really similar. We need to *warp time* to account for this shift!

In the visualizations below^[https://www.r-bloggers.com/time-series-matching-with-dynamic-time-warping/], 
you can see how dynamic time warping stretches
(warps) time to match up nearby points. 

```{r}
plot_dtw <- function(ts1, ts2, ...) {
  dtw::dtw(ts1, ts2) %>%
    dtw::dtwPlotTwoWay(xts = ts1, yts = ts2, ...)
}

plot_dtw(
  eth_sample_trunc$ts[[1]], eth_sample_trunc$ts[[2]], col = c("#F8766D", "#619CFF")
)
```

When comparing red to green below, 
there is a lot more warping going on to match up 
points (as measured by the light gray concentric lines between the series),
so the time series are more dissimilar.

```{r}
plot_dtw(
  eth_sample_trunc$ts[[1]], eth_sample_trunc$ts[[3]], 
  col = c("#F8766D", "#00BA38")
)
```

The dissimilarity between red and green is reflected when we
calculate the dynamic time warping distance.

```{r}
eth_sample$ts %>% 
  set_names(c("red", "blue", "green")) %>%
  simplify2array() %>% t() %>%
  dist(method = "DTW")
```


## Clustering Time Series

Equipped with a measure of similarity, we can now attempt to cluster
the time series. The time series studied in previous examples are part of a set of
504 time series, belonging to four classes. 

I'll use `step_dtw` from my tsrecipes package to cluster using the dynamic time
warping similarity metric. `step_dtw` uses the excellent 
[dtwclust](https://github.com/asardaes/dtwclust) package behind the scenes.

```{r}

prepped <- recipe(ethanol) %>%
    step_dtw(ts, k = 4) %>%
    prep() 

 prepped$steps[[1]]$dtwclust$ts@distmat %>% 
   saveRDS("ethanol_distmat.RDS")


if (!file.exists("ethanol_clusters.RDS")) {
  ethanol_clusters <- recipe(ethanol) %>%
    step_dtw(ts, k = 4) %>%
    prep() %>%
    bake(ethanol)
  
  saveRDS(ethanol_clusters, "ethanol_clusters.RDS")
}

ethanol_clusters <- readRDS("ethanol_clusters.RDS")
```




With clustering, I think it's important to evaluate the clusters using
*objective* and *subjective* criteria. 

Subjective criteria include

* visualizing the "shape" of time series within clusters to see if there is a
  pattern. If the shape isn't obvious, you can try alternative methods or
  increase the number of clusters. Visualizations of noisy, high-frequency
  time series may not be useful. In this case, you may want to visualize 
  smoothed trends of the cluster, rather than raw time series.

* inspecting clusters for clutter: elements within the cluster that don't seem
  to belong. This may indicate you need to increase the number of clusters.

Objective criteria include

* checking the number of elements per cluster.
  Especially with hierarchical clustering, occasionally a cluster will have
  90% of the data, which isn't very useful.
  
* evaluation against known classes. If working with unlabeled data, sometimes 
  there may be a small portion of labeled data to evaluate against.
  
* calculating cluster [statistics](https://uc-r.github.io/hc_clustering#optimal).

Four classes are known ahead of time, so it seems reasonable to start with 
four clusters.

I always start with visualizing the time series within each cluster.

```{r}
ethanol_clusters %>%
  rowwise() %>%
  mutate(n = list(1:1751)) %>%
  ungroup() %>%
  unnest(c(ts, n)) %>%
  ggplot() +
  geom_line(aes(n, ts, color = class, group = id), show.legend = FALSE) +
  facet_wrap(~dtwclust_ts)
```

Visually, there are distinct shapes within each cluster. Clusters 2 and 3
both have a clear middle dip. Cluster 1 has very few wiggles on the right, and
cluster 4 is almost its mirror image, with few wiggles on the left. 

All clusters seem a little clutter: especially 1 and 4 with all the wiggles
on the left and right respectively. 

Comparing the "shape" of the clusters to the shape of the individual classes,
there doesn't seem to be a lot of obvious similarity.

```{r}
ethanol_clusters %>%
  rowwise() %>%
  mutate(n = list(1:1751)) %>%
  ungroup() %>%
  unnest(c(ts, n)) %>%
  ggplot(aes(n, ts, color = as.factor(dtwclust_ts))) +
  geom_line(aes(group = id), show.legend = FALSE) +
  facet_wrap(~class)
```

The actual classes of the time series do not visually group into distinct 
shapes, indicating to me there is a lot of variation within each class.

```{r}
mm_model <- ethanol_clusters %>%
  mutate(dtwclust_ts = as.factor(dtwclust_ts)) %>%
  nnet::multinom(class ~ dtwclust_ts, data = .)
```

Predicting the class based on the cluster is only 31% accurate. Better than 
random chance (25%), but still not great. 

```{r}
pred_eth <- ethanol_clusters %>%
  mutate(dtwclust_ts = as.factor(dtwclust_ts)) %>%
  mutate(pred = predict(mm_model, ., type = "class"))

pred_eth %>%
  group_by(class, pred) %>%
  summarise(n = n()) %>%
  group_by(pred_correct = class == pred) %>%
  summarise(n = sum(n)) %>%
  mutate(percent = n / sum(n))
```


Based on this analysis, I bet the clustering will be more useful if there are
more clusters. While each cluster has a unique shape, there is still a lot of 
clutter. Moreover, we see a large amount of variation within each class. 
There will need to be more clusters to capture that variation.



## TODO

One of the most expensive parts of `tsclust` is calculating the distance matrix.
If we could presupply the distance matrix as an option to step_dtw 
(or another step entirely), then we could significantly speed up the process.

what about `step_proxy` and use different clustering methods against that?



```{r}
if (!file.exists("ethanol_clusters8.RDS")) {
  ethanol_clusters8 <- recipe(ethanol) %>%
    step_dtw(ts, k = 8) %>%
    prep() %>%
    bake(ethanol)
  
  saveRDS(ethanol_clusters8, "ethanol_clusters8.RDS")
}

ethanol_clusters8 <- readRDS("ethanol_clusters8.RDS")
```

```{r}
model8 <- ethanol_clusters8 %>%
  mutate(dtwclust_ts = as.factor(dtwclust_ts)) %>%
  nnet::multinom(class ~ dtwclust_ts, data = .)
```

With 8 clusters, there are more definition to the shapes, and a lot less clutter.

```{r}
ethanol_clusters8 %>%
  rowwise() %>%
  mutate(n = list(1:1751)) %>%
  ungroup() %>%
  unnest(c(ts, n)) %>%
  ggplot() +
  geom_line(aes(n, ts, color = class, group = id), show.legend = FALSE) +
  facet_wrap(~dtwclust_ts)
```

```{r}
pred_eth <- ethanol_clusters8 %>%
  mutate(dtwclust_ts = as.factor(dtwclust_ts)) %>%
  mutate(pred = predict(model8, ., type = "class"))

pred_eth %>%
  group_by(class, pred) %>%
  summarise(n = n()) %>%
  group_by(pred_correct = class == pred) %>%
  summarise(n = sum(n)) %>%
  mutate(percent = n / sum(n))
```

Additionally, there is a 3% increase in accuracy. 

### Additional Clusters

You may have noticed that running `step_dtw` takes a long time. 
The bottleneck is calculating the dynamic time warping distance. 
Most implementations
have a computational complexity of $O(N^2)$^[
Some improvements can be made. `dtwclust` offers `dtw_basic` by default, 
which is significantly faster, with fewer features. 
And the [theoretical](https://dl.acm.org/doi/10.1145/3230734)
computational complexity is $O(n^2/\log\log(n))$, although I don't know
if this has been implemented anywhere, or if its technically feasible to do so.
] 
to calculate the distance between two time series, 
and that calculation must happen between *every pair* of time series 
in the dataset to cluster.

Fortunately, the `dtwclust` interface lets you precompute the the similarity
matrix and supply that to the cluster algorithms. Care must be taken here
to avoid data leakage (see the section below).










Finding useful clusters requires setting enough distinct clusters.

But finding these clusters is also sensitive to the many options
available in `dtwclust::tsclust`. 

Theoretically, you could tune across the number of clusters, 
as well as cluster (and distance methods). 

It's worth checking out `dtwclust::compare_clusterings` if you are
interested in doing this. 
Right now it's not supported in `step_dtw`. Clustering with `tsclust`
takes a long time, and finding useful clusters can also be subjective.

I'd strongly recommend plotting clusters as a part of exploratory data analysis,
rather than tuning blindly. Whether you doing a single clustering, or
evaluating clusters objectively (scoring against classes) or subjectively
(looking at the shape of clusters), you need to limit your explorations
to the train set.

Just make sure you are still only using your
training data,  not your test data, when subjectively evaluating your clusters
to avoid information leakage. 

Be careful here: preprocessing, and even exploration on the training set
can create [information leakage](http://www.feat.engineering/resampling.html),
make your model appear more effective than it actually is.


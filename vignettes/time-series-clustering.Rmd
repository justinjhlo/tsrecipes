---
title: "Time series clustering with dynamic time warping"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{time-series-clustering}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

Clustering is all about finding groups of similar observations based on 
their features. It's used for

* exploratory data analysis. Especially useful for time series where you can 
  cluster time series of similar "shape".

* feature engineering for time series classification, using the cluster as 
  a feature. Can also be thought of as a dimensionality reduction
  technique.

* labeling groups of time series.

In this guide, I'll show how you can use tsrecipes to cluster
time series with dynamic time warping using the 
[dtwclust](https://github.com/asardaes/dtwclust)
R package^[
You can also cluster time series using the discrete cosine transform coefficients
found using `step_dct`. These features are uncorrelated, so you are
free to use traditional distance metrics and clustering techniques.
].

If you'd like to learn more about dynamic time warping,
check out my [dynamic time warping](https://uc-r.github.io/kmeans_clustering)
article.

For an introduction to clustering in general,
UC Business Analytics Programming Guide has an excellent 
[series](https://uc-r.github.io/kmeans_clustering)
on clustering, introducing distance metrics and clustering techniques.

## Evaluating Clusters

With clustering, I think it's important to evaluate the clusters using
*objective* and *subjective* criteria. 

Subjective criteria include

* visualizing the "shape" of time series within clusters to see if there is a
  pattern. If the shape isn't obvious, you can try alternative methods or
  increase the number of clusters. Visualizations of noisy, high-frequency
  time series may not be useful. In this case, you may want to visualize 
  smoothed trends of the cluster, rather than raw time series.

* inspecting clusters for clutter: elements within the cluster that don't seem
  to belong. This may indicate you need to increase the number of clusters.

Objective criteria include

* checking the number of elements per cluster.
  Especially with hierarchical clustering, occasionally a cluster will have
  90% of the data, which isn't very useful.
  
* evaluation against known classes, if available. This can even be helpful
  if only a small amount of labeled data is available.
  
* calculating cluster statistics^[I won't cover clustering statistics here,
  but the UC Business Analytics Programming Guide 
  [series](https://uc-r.github.io/kmeans_clustering)
  has many examples.].

These principles will serve as a guide while clustering the
[Ethanol](http://www.timeseriesclassification.com/description.php?Dataset=EthanolLevel)
data.

## Ethanol

The `ethanol` dataset has four classes, based on the levels of alcohol in the 
fuel. 


```{r}
library(tsrecipes)
library(tidyverse)
library(dtwclust)
library(tidymodels)
```

```{r}
ethanol %>%
  rowwise() %>%
  mutate(n = list(1:1751)) %>%
  ungroup() %>%
  unnest(c(ts, n)) %>%
  ggplot(aes(n, ts)) +#, color = as.factor(id))) +
  geom_line(aes(group = id), alpha = 0.2, show.legend = FALSE) +
  facet_wrap(~class)
```

Visually, it's hard for me to distinguish between classes. 
Maybe this indicates it will be hard to classify, or cluster into meaningful
groups.

Four classes are known ahead of time, so it seems reasonable to start with 
four clusters.

## Clustering time series

`step_dtw` clusters time series using the dynamic time
warping similarity metric. Behind the scenes, `step_dtw` uses
[dtwclust](https://github.com/asardaes/dtwclust). All it's options
are available, but we'll stick with the defaults.

```{r, echo=FALSE}
if (!file.exists("ethanol_distmat.RDS")) {
  prepped <- recipe(ethanol) %>%
      step_dtw(ts, k = 4) %>%
      prep() 
  
   prepped$steps[[1]]$dtwclust$ts@distmat %>% 
     saveRDS("ethanol_distmat.RDS")
}

if (!file.exists("ethanol_clusters.RDS")) {
  ethanol_clusters <- recipe(ethanol) %>%
    step_dtw(ts, k = 4) %>%
    prep() %>%
    bake(ethanol)
  
  saveRDS(ethanol_clusters, "ethanol_clusters.RDS")
}

ethanol_clusters <- readRDS("ethanol_clusters.RDS")
```

```{r, eval=FALSE}
prepped <- recipe(ethanol) %>%
  step_dtw(ts, k = 4) %>%
  prep() 

ethanol_clusters <- bake(prepped, ethanol)
```

I always start with visualizing the time series within each cluster.

```{r}
ethanol_clusters %>%
  rowwise() %>%
  mutate(n = list(1:1751)) %>%
  ungroup() %>%
  unnest(c(ts, n)) %>%
  ggplot() +
  geom_line(aes(n, ts, group = id), alpha = 0.2, show.legend = FALSE) +
  facet_wrap(~dtwclust_ts)
```

I see distinct shapes within each cluster. Clusters 1 and 4
both have a clear middle dip, with different wiggles leading the the middle.
Cluster 2 has very few wiggles on the right, and
cluster 3 is sort of its mirror image, with few wiggles on the left. 

All clusters seem a little cluttered: especially 2 and 3
with all the wiggles on the left and right respectively. Cluttered clusters
mean we might get more logical groups if we increase the number of clusters.

Comparing the "shape" of the clusters to the shape of the individual classes
from the previous section,
there doesn't seem to be a lot of obvious similarity. 
There's a lot more variation and a lot less structure to the class plots. 

### Evaluation

The clusters appear logically grouped, but they don't necessarily look like the
classes. Fortunately the `ethanol` dataset is labeled. We can test
to see if the groups are predictive of the outcome.

```{r}
mm_model <- ethanol_clusters %>%
  mutate(dtwclust_ts = as.factor(dtwclust_ts)) %>%
  nnet::multinom(class ~ dtwclust_ts, data = .)
```

```{r}
pred_eth <- ethanol_clusters %>%
  mutate(dtwclust_ts = as.factor(dtwclust_ts)) %>%
  mutate(pred = predict(mm_model, ., type = "class"))

pred_eth %>%
  group_by(class, pred) %>%
  summarise(n = n()) %>%
  group_by(pred_correct = class == pred) %>%
  summarise(n = sum(n)) %>%
  mutate(percent = n / sum(n))
```

Predicting the class based on the cluster is only 31% accurate. Better than 
random chance (25%), but still not great. 

Based on this analysis, I bet the clustering will be more predictive of the class
if there are more clusters. 

## Tuning number of clusters

If you are following along, you may have noticed that clustering
takes a long time.

One of the most expensive parts is calculating the distance matrix^[
Also called the dissimilarity matrix.
]. This must be done for every pair of observations. For 500 observations,
that's 124,750 unique pairs^[500 choose 2].

For each pair, you also need to calculate the dynamic time warping distance,
which has a computational complexity of $O(N^2)$^[
Some improvements can be made. `dtwclust` offers `dtw_basic` by default, 
which is significantly faster, with fewer features. 
And the [theoretical](https://dl.acm.org/doi/10.1145/3230734)
computational complexity is $O(n^2/\log\log(n))$, although I don't know
if this has been implemented anywhere, or if its technically feasible to do so.
].

Fortunately, the `dtwclust::tsclust` lets you precompute the the similarity
matrix and supply that to the cluster algorithms. Care must be taken here
to avoid data leakage; make sure the distance matrix is only computed
on the training data.

Using the precomputed distance matrix, it's possible
to tune the number of clusters much faster than otherwise
(although not necessarily recommended: see the next section).

```{r, echo=FALSE}
distmat <- readRDS("ethanol_distmat.RDS")
```

```{r, eval=FALSE}
distmat <- prepped$steps[[1]]$dtwclust$ts@distmat
```

```{r}
dtw_options = list(control = dtwclust::partitional_control(distmat = distmat))

rec <- recipe(ethanol, var = names(ethanol), roles = c("id", "outcome", "input")) %>%
  step_dtw(ts, k = tune(), options = dtw_options) %>%
  step_mutate_at(all_predictors(), fn = factor)
```

```{r}
validation_set <- tibble(
  splits = list(make_splits(
    list(analysis = 1:504, assessment = 1:504), data = ethanol
  ))
) %>%
  new_rset(c(id = "validation"), subclass = "rset")
```

```{r, eval=FALSE}
tune_results <- workflow() %>%
  add_model(multinom_reg() %>% set_engine("nnet")) %>%
  add_recipe(rec) %>%
  tune_grid(
    resamples = validation_set,
    grid = expand_grid(k = c(4, 8, 16, 32, 64))
  )
```

```{r, echo=FALSE}
if (!file.exists("ethanol_tune_results.RDS")) {
  tune_results <- workflow() %>%
    add_model(multinom_reg() %>% set_engine("nnet")) %>%
    add_recipe(rec) %>%
    tune_grid(
      resamples = validation_set,
      grid = expand_grid(k = c(4, 8, 16, 32, 64))
    )
  saveRDS(tune_results, "ethanol_tune_results.RDS")
}

tune_results <- readRDS("ethanol_tune_results.RDS")
```

```{r}
tune_results %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(k, mean)
```

Increasing the number of clusters does make the feature more useful in
predicting the outcome. With too many clusters, the usefulness
starts to decrease. There may be more correlation with the class,
but you can no longer visualize and understand why certain time series
are clustered. 

If the goal is to simplify increase the classification accuracy,
rather than understanding similar groups, there are better and faster ways.

## Alternatives to supervised clustering

Tuning clusters is not necessarily something I'd recommend.
Clustering takes a long time, even if we are using the precomputed
distance matrix. 

And there are much better options for dimensionality reduction 
for time series. Predicting the class using 8 discrete cosine transform coefficients 
is almost as accurate as predicting the class with 64 clusters.
32 coefficients brings the accuracy to 92%.

```{r}
rec <- recipe(ethanol, var = names(ethanol), roles = c("id", "outcome", "input")) %>%
  step_dct(ts, k = tune())

tune_results <- workflow() %>%
  add_model(multinom_reg() %>% set_engine("nnet")) %>%
  add_recipe(rec) %>%
  tune_grid(
    resamples = validation_set,
    grid = expand_grid(k = c(4, 8, 16, 32, 64))
  )

tune_results %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(k, mean)
```

## Additional cluster options

Many clustering techniques and options are available in
available in `dtwclust::tsclust`. I recommend reading the 
[documentation](https://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf)
if you aren't satisified with the defaults.

Additionally, `step_dtw` only allows tuning the number the number of clusters,
but if you want more flexibility look into `dtwclust::compare_clusterings`.
